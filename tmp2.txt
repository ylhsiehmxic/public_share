Developing a RAID Bdev for SPDK
1. Addressing the Write-Hole Problem with Software-Based Consistency
The write-hole problem is a fundamental challenge in RAID systems, particularly those
using parity-based schemes like RAID 5 and RAID 6. It arises from the non-atomic
nature of stripe updates across multiple independent disks. When a write operation
modifies a data block within a stripe, the corresponding parity block must also be
updated to maintain consistency. However, if a system crash or power failure occurs
after some, but not all, of the stripe's blocks (data and parity) have been written, the
stripe enters an inconsistent state. Upon recovery, if the array is also degraded (i.e.,
one or more disks have failed), this inconsistency can lead to silent data corruption.
Reconstructing a lost data block using the now-inconsistent parity will yield incorrect
data . This section explores several software-based mechanisms to address this
critical issue, focusing on solutions that can be implemented using generic NVMe SSDs
without requiring specialized hardware features. These methods, primarily centered
around journaling and logging, provide a robust framework for ensuring data
consistency even in the face of unexpected failures.
1.1. Leveraging Journaling and Logging Mechanisms
Journaling, a concept borrowed from database management systems and modern file
systems, offers a well-established solution to the write-hole problem. The core
principle is to treat a set of related write operations as a single, atomic transaction.
Before any data is written to its final location on the main storage array, a description
of the intended changes is first recorded in a dedicated, persistent log, often referred
to as a journal or write-ahead log (WAL) . This ensures that there is always a complete
record of in-flight operations. In the event of a crash, the system can recover by
examining the log. If a transaction was committed to the log but not yet fully applied to
the main array, the system can replay the transaction from the log to complete the
operation, thereby restoring consistency. If a crash occurred before the transaction was
committed to the log, the partial changes are simply discarded, leaving the array in a
consistent state. This approach effectively transforms a multi-step, non-atomic write
process into a logically atomic one from the perspective of crash recovery.
1.1.1. Implementing a Dedicated Journal Device
Generated by Kimi AI
One of the most straightforward approaches to implementing journaling for a RAID
array is to use a separate, dedicated device for the journal. This device, often a highperformance, low-latency SSD or even a persistent memory module, is used exclusively
for storing the write-ahead log. The primary advantage of this method is that it
isolates the high-frequency, small, and sequential writes of the journal from the
larger, more random I/O patterns of the main data array. This separation can
significantly improve performance, as the journal writes do not contend for I/O
bandwidth with the data disks. Furthermore, because the journal is on a separate
device, it can be optimized for the specific workload of logging, such as by using a
high-endurance SSD to withstand the frequent write cycles.
The Linux kernel's MD RAID implementation provides a practical example of this
approach. It allows for the configuration of a journal device, which can be any block
device, including a mirrored pair of SSDs to avoid creating a single point of failure .
When a write request is received by the RAID engine, instead of immediately writing the
data and parity to the main array, the entire stripe (or a description of the changes) is
first written to the journal device as part of a transaction. Once the transaction is
safely committed to the journal, the write is then allowed to proceed to the main array.
After the write to the main array is confirmed to be complete, the corresponding entry
in the journal is marked as free, allowing the space to be reused in a circular log
fashion. In the event of a crash, the system restarts, reads the journal, and replays any
uncommitted transactions, ensuring the array is brought to a consistent state without
requiring a full resynchronization . While this method is effective, it does require an
additional storage device, which adds to the cost and complexity of the system.
1.1.2. Adopting an Embedded Journal within the RAID Volume
An alternative to a dedicated journal device is to embed the journal within the RAID
volume itself. This approach avoids the need for additional hardware by carving out a
specific region on the member disks of the array to serve as the log. This method is
more space-efficient and can be simpler to manage, as there is no separate physical
device to provision and monitor. However, it introduces a trade-off, as the journal I/O
now competes directly with the data I/O on the same set of disks. This can
potentially lead to performance degradation, especially under heavy write loads, as the
heads of traditional hard drives or the I/O channels of SSDs must service both the
journal and the data requests.
Despite the potential for contention, this approach has been successfully implemented
in various storage systems. The key to making it performant is to carefully manage the
Generated by Kimi AI
I/O scheduling and to design the journal structure to minimize its impact on data I/O.
For example, the journal can be implemented as a circular buffer with a fixed size, and
writes to the journal can be batched to reduce the number of individual I/O operations.
The Linux MD RAID's "raid5-cache" feature is an example of a solution that uses the
array's own disks to manage a form of journaling to close the write hole, although it is
distinct from the Partial Parity Log (PPL) method . The effectiveness of an embedded
journal is highly dependent on the specific workload and the performance
characteristics of the underlying storage devices. For NVMe SSDs, which offer high
IOPS and low latency, the contention might be less of a concern compared to
traditional hard drives, making an embedded journal a more viable option.
1.1.3. Utilizing a Partial Parity Log (PPL) for RAID 5
The Partial Parity Log (PPL) is a more sophisticated and efficient logging mechanism
specifically designed to address the write-hole problem in RAID 5 arrays. Unlike a full
journal that might record entire data blocks, the PPL only stores the "partial parity" for
a stripe. The partial parity is the XOR of the data chunks within a stripe that are not
being modified by the current write operation. This is a crucial insight: by storing this
value, the system can recover the original state of the stripe before the write began,
regardless of which specific data or parity writes were completed before the crash .
This method is more space-efficient than a full journal because it only stores a single
parity chunk per in-flight write operation, rather than all the data chunks involved.
The PPL is implemented as a distributed log, meaning that the log entries are stored
directly on the member drives of the RAID array, typically in a reserved metadata area
on the parity drive of each stripe. This eliminates the need for a dedicated journal
device and avoids the single point of failure it could represent. When a write request is
handled, the PPL entry (the partial parity) is written to the log before the new data and
parity are dispatched to the disks. If a crash occurs and the array is restarted, the PPL
is inspected. For any stripe with an outstanding PPL entry, the system can use the
partial parity to reconstruct the data that was not being written, thereby restoring the
stripe to a consistent state. This recovery process is performed during the array startup
and eliminates the need for a full array resynchronization, which can be a timeconsuming process . The Linux kernel's MD RAID implementation has adopted PPL as a
feature for RAID5 arrays, demonstrating its viability as a software-only solution for the
write-hole problem .
1.2. Exploring NVMe Feature Integration for Atomicity
Generated by Kimi AI
While journaling provides a robust software-based solution, leveraging specific
features of the NVMe protocol can offer more efficient, hardware-assisted mechanisms
for ensuring write atomicity. The NVMe standard includes several features that can be
exploited to build a more performant and reliable RAID bdev. These features, such as
atomic write commands and persistent memory regions, are designed to provide
guarantees about the durability and consistency of data, which can be directly applied
to solve the write-hole problem. By integrating these NVMe features into the RAID
logic, it is possible to reduce the software overhead associated with traditional
journaling and to build a more streamlined and efficient consistency mechanism. This
approach requires a deeper understanding of the NVMe specification and careful
programming to ensure that the features are used correctly and effectively.
1.2.1. Using NVMe Atomic Write Commands
The NVMe specification includes commands that can be used to ensure the atomicity
of write operations. An atomic write guarantees that either all of the data in a
command is written to the device, or none of it is. This is a powerful feature for
building consistent storage systems, as it can be used to prevent the partial write
scenarios that lead to the write-hole problem. For example, a RAID bdev could be
designed to issue a single, large atomic write command that encompasses all the data
and parity blocks for a given stripe. If the command completes successfully, the stripe
is guaranteed to be consistent. If the command fails or is interrupted by a crash, the
NVMe device guarantees that no partial data was written, leaving the stripe in its
original, consistent state.
However, the practical application of NVMe atomic writes in a RAID context can be
challenging. The size of an atomic write is often limited by the NVMe device and the
underlying hardware. For large stripe sizes, it may not be possible to fit all the data and
parity blocks into a single atomic write command. In such cases, a more complex
protocol would be needed, potentially involving multiple atomic writes and a final
"commit" command to make the changes durable. Furthermore, not all NVMe devices
may support the same level of atomicity guarantees, so the implementation would
need to be carefully tailored to the specific capabilities of the target hardware. Despite
these challenges, the use of NVMe atomic writes represents a promising avenue for
building a high-performance, crash-consistent RAID bdev, as it has the potential to
eliminate the need for a separate journaling mechanism altogether.
1.2.2. Leveraging Persistent Memory Regions (PMR) in NVMe SSDs
Generated by Kimi AI
Some NVMe SSDs include a Persistent Memory Region (PMR) , which is a region of
the device's memory that is persistent across power cycles and can be accessed
directly by the host system. This PMR can be used as a high-performance, byteaddressable storage medium, similar to NVDIMM-N. The PMR can be leveraged to
implement a very fast and efficient journaling mechanism for the RAID bdev. Instead of
writing journal entries to a separate SSD or to a reserved area on the main data disks,
the RAID engine could write them to the PMR of one or more of the NVMe devices in
the array.
The use of PMR for journaling offers several advantages. First, it provides extremely
low latency and high bandwidth, as the journal writes are performed directly to the
device's internal memory, bypassing the traditional block I/O stack. This can
significantly reduce the performance overhead of journaling, especially for small,
frequent write operations. Second, because the PMR is persistent, the journal entries
will survive a power failure, ensuring that the system can recover correctly. The RAID
bdev could be designed to write transaction metadata and even the partial parity
information (for a PPL-like mechanism) to the PMR. Upon recovery, the system would
simply read the journal from the PMR and replay any uncommitted transactions. This
approach combines the robustness of journaling with the performance of persistent
memory, offering a compelling solution for the write-hole problem in high-performance
all-flash arrays.
1.2.3. Embedding Transaction Information in NVMe Command Fields
The NVMe command structure includes several reserved and vendor-specific fields that
can be used to embed additional information. A RAID bdev could use these fields to
carry transaction identifiers, sequence numbers, or other metadata that is necessary
for ensuring consistency. For example, when issuing a write command for a stripe, the
RAID bdev could embed a unique transaction ID in a reserved field of the NVMe
command. This ID would be the same for all writes (data and parity) that are part of the
same stripe update. Upon recovery, the system could scan the drives for commands
with the same transaction ID to identify incomplete stripe updates and take corrective
action. This approach is similar to the one used by ccNVMe , which embeds
transaction order information in the command fields . This method has the advantage
of being compatible with standard NVMe drives, as it does not require any special
hardware features. However, it requires careful design to ensure that the embedded
information is sufficient for recovery and that the overhead of processing this metadata
does not significantly impact performance.
Generated by Kimi AI
1.3. Adopting Distributed Systems Principles for Local Consistency
The problem of ensuring consistency across multiple independent storage devices is
not unique to RAID systems. It is a fundamental challenge in distributed systems, where
data is often replicated across multiple nodes. Many of the techniques developed for
distributed systems, such as quorum-based protocols, versioned data, and distributed
logging, can be adapted to solve the write-hole problem in a local RAID array. By
treating each NVMe drive as a node in a distributed system, it is possible to build a
RAID bdev that is both highly available and strongly consistent. This approach has the
advantage of being well-understood and widely used, with a rich body of research and
practical experience to draw from. It also has the potential to be more scalable and
resilient than traditional RAID implementations, as it can be designed to handle not just
single-drive failures, but also more complex failure scenarios.
1.3.1. Implementing a Quorum-Based Commit Protocol
A quorum-based commit protocol is a distributed consensus algorithm that ensures
that a transaction is only committed if a majority of the nodes in the system agree to it.
In the context of a RAID bdev, this would mean that a stripe write is only considered
complete if a majority of the drives in the array have successfully written their
respective data or parity chunks. This approach can be used to prevent the write-hole
problem by ensuring that there is always a consistent copy of the data available, even if
some of the drives fail. For example, in a RAID 5 array with five drives, a quorum could
be defined as three drives. When a stripe write is initiated, the RAID bdev would send
the write requests to all five drives. The write would only be considered successful if at
least three of the drives acknowledge the write. If a crash occurs, the system can
recover by reading the data from the three drives that form a quorum and using the
parity information to reconstruct the data on the failed drives. This approach is more
complex to implement than traditional RAID, as it requires a more sophisticated
communication and coordination mechanism between the drives. However, it can
provide a higher level of availability and consistency, making it a promising solution for
high-performance, all-flash arrays.
1.3.2. Using Versioned Metadata and Checksums for Recovery
Another technique that can be borrowed from distributed systems is the use of
versioned metadata and checksums to detect and recover from inconsistencies. In this
approach, each data and parity chunk is associated with a version number and a
checksum. The version number is incremented every time the chunk is modified, and
Generated by Kimi AI
the checksum is calculated over the data and the version number. When a stripe write
is initiated, the RAID bdev would first read the current version numbers and checksums
for all the chunks in the stripe. It would then calculate the new parity, increment the
version numbers, and calculate the new checksums. The new data, parity, version
numbers, and checksums would then be written to the drives. If a crash occurs, the
system can recover by reading the version numbers and checksums for all the chunks
in the stripe. If the version numbers and checksums are consistent, the stripe is
considered to be in a consistent state. If they are inconsistent, the system can use the
parity information to reconstruct the missing or corrupted data. This approach has the
advantage of being relatively simple to implement and does not require a separate
journal device. However, it does require additional storage space for the version
numbers and checksums, and it can be less efficient than journaling, as it requires
reading the old data before writing the new data.
2. Eliminating Lock-Induced Performance Stalls in the RAID Bdev
In high-performance storage systems, lock-induced performance stalls are a major
bottleneck. Traditional storage software, such as the Linux kernel's block layer, relies
heavily on locks to protect shared data structures. While this approach ensures
correctness, it can lead to significant performance degradation in multi-core
environments, as threads compete for access to the same locks. The Storage
Performance Development Kit (SPDK) was designed to address this problem by
providing a user-space, lock-free architecture that can scale to a large number of
cores. By building the new RAID bdev on top of SPDK, it is possible to eliminate most
of the lock-induced performance stalls and achieve a high level of parallelism.
However, the RAID bdev itself must also be designed to be lock-free, as it will have its
own internal data structures and I/O paths that need to be protected. This requires a
careful design that leverages SPDK's core principles, such as the reactor model,
message passing, and lock-free data structures.
2.1. Leveraging SPDK's Lock-Free Architecture
SPDK's architecture is built around the concept of a "reactor," which is a user-space
thread that is pinned to a specific CPU core. Each reactor has its own set of resources,
such as memory and I/O queues, and it processes events in a single-threaded, run-tocompletion manner. This design eliminates the need for locks within a reactor, as there
is only one thread accessing the data structures. Communication between reactors is
handled through a message-passing mechanism, which uses lock-free ring buffers to
Generated by Kimi AI
transfer events between cores. This approach allows the system to scale to a large
number of cores without the performance degradation associated with lock contention.
By designing the RAID bdev to be compatible with this architecture, it is possible to
achieve a high level of parallelism and minimize lock-induced performance stalls.
2.1.1. Utilizing the Reactor Model for Per-Core Processing
The reactor model is the cornerstone of SPDK's lock-free architecture. By assigning a
separate reactor to each CPU core, the system can process I/O requests in parallel
without the need for locks. In the context of a RAID bdev, this means that each reactor
can be responsible for handling a subset of the I/O requests. For example, the I/O
requests could be partitioned based on the LBA, with each reactor handling a specific
range of LBAs. This would ensure that there is no contention between reactors for
access to the same data. The RAID bdev would need to be designed to support this
partitioning, with each reactor having its own set of data structures for managing the
I/O requests and the state of the RAID array. This approach has the advantage of being
highly scalable, as the performance of the system can be increased by simply adding
more CPU cores. However, it also requires a more complex design, as the RAID bdev
must be able to handle the distribution of I/O requests across multiple reactors and the
coordination of recovery operations in the event of a failure.
2.1.2. Implementing Lock-Free Data Structures (e.g., Ring Buffers)
Lock-free data structures are a key component of SPDK's architecture. They allow
multiple threads to access the same data structure without the need for locks, which
can significantly improve performance in multi-core environments. The most common
lock-free data structure used in SPDK is the ring buffer, which is used for
communication between reactors. In the context of a RAID bdev, lock-free data
structures could be used to manage the I/O request queues, the free lists for memory
allocation, and the state of the RAID array. For example, a lock-free ring buffer could
be used to pass I/O requests from the application to the RAID bdev, and another ring
buffer could be used to pass the completed requests back to the application. This
would eliminate the need for locks in the I/O path, which is a common source of
performance bottlenecks. The implementation of lock-free data structures is a complex
topic, and it requires a deep understanding of the memory model of the CPU and the
compiler. However, SPDK provides a set of well-tested and highly optimized lock-free
data structures that can be used as a starting point for the development of the RAID
bdev.
Generated by Kimi AI
2.1.3. Using Message Passing for Inter-Reactor Communication
Message passing is the primary mechanism for communication between reactors in
SPDK. It allows reactors to exchange information and coordinate their actions without
the need for shared memory and locks. In the context of a RAID bdev, message passing
could be used to handle a variety of tasks, such as distributing I/O requests,
coordinating recovery operations, and managing the state of the RAID array. For
example, when a write request is received, the RAID bdev could send a message to the
reactor that is responsible for the corresponding LBA range. The reactor would then
process the request and send a message back to the original reactor when the request
is complete. This approach has the advantage of being highly scalable and resilient, as
it does not rely on any shared state between the reactors. However, it also requires a
more complex design, as the RAID bdev must be able to handle the asynchronous
nature of message passing and the potential for messages to be lost or reordered.
2.2. Optimizing I/O Path and Resource Management
In addition to eliminating locks, it is also important to optimize the I/O path and
resource management in the RAID bdev. This involves minimizing the number of
memory copies, reducing the overhead of system calls, and ensuring that the system
has enough resources to handle the I/O load. SPDK provides a number of features that
can be used to optimize the I/O path, such as zero-copy I/O, huge pages, and a userspace TCP/IP stack. By leveraging these features, it is possible to build a RAID bdev
that can achieve a very high level of performance, even under heavy load.
2.2.1. Adopting a Run-to-Completion (RTC) Model
The run-to-completion (RTC) model is a programming model where a task is executed
from start to finish without being preempted or blocked. This model is well-suited for
high-performance I/O systems, as it eliminates the overhead of context switching and
synchronization. In the context of a RAID bdev, the RTC model could be used to
process I/O requests. When a request is received, it would be processed by a single
reactor from start to finish, without being passed between multiple threads or
processes. This would minimize the latency of the I/O operation and ensure that the
system can handle a high rate of requests. The main challenge with the RTC model is
that it can be difficult to implement for complex tasks that require access to shared
resources or that need to wait for external events. However, by using a combination of
the RTC model and message passing, it is possible to build a RAID bdev that is both
highly efficient and scalable.
Generated by Kimi AI
2.2.2. Using I/O Channels to Avoid Resource Contention
I/O channels are a mechanism in SPDK for managing the I/O resources of a device.
They provide a way to create multiple, independent I/O queues for a single device,
which can be used to avoid resource contention and improve performance. In the
context of a RAID bdev, I/O channels could be used to create a separate I/O queue for
each reactor. This would ensure that there is no contention between reactors for
access to the same I/O queue, which can be a major source of performance
bottlenecks. The RAID bdev would need to be designed to support the creation and
management of I/O channels, and it would need to be able to distribute the I/O
requests across the different channels. This approach has the advantage of being
highly scalable, as the performance of the system can be increased by simply adding
more I/O channels. However, it also requires a more complex design, as the RAID bdev
must be able to handle the management of the I/O channels and the distribution of the
I/O requests.
2.2.3. Implementing Priority Queues for Critical I/O (e.g., Parity Writes)
In a RAID system, not all I/O operations are created equal. Some operations, such as
parity writes, are more critical than others, as they are required to maintain the
consistency of the array. If a parity write is delayed, it can lead to a situation where the
array is vulnerable to data loss in the event of a drive failure. To address this, it is
possible to implement a priority queue in the RAID bdev to prioritize critical I/O
operations. This would ensure that parity writes and other critical operations are
processed before less critical operations, such as background reads or non-critical
writes. The implementation of a priority queue in a lock-free environment is a complex
task, but it can be achieved by using a combination of lock-free data structures and a
careful design. For example, a separate lock-free ring buffer could be used for each
priority level, and the reactor could be designed to process the high-priority ring buffer
before the low-priority ring buffer. This approach has the advantage of being highly
flexible, as the priority of each I/O operation can be determined based on a variety of
factors, such as the type of operation, the LBA, or the application that initiated the
request.
2.3. Learning from Existing High-Performance Storage Systems
There are many existing high-performance storage systems that can be used as a
source of inspiration for the design of the new RAID bdev. These systems have been
developed over many years and have been optimized for a wide range of workloads and
Generated by Kimi AI
hardware platforms. By studying the design of these systems, it is possible to learn
from their successes and failures and to avoid making the same mistakes. Some of the
most relevant systems to study include Ceph, ScalaAFA, and xiRAID.
2.3.1. Ceph's Approach to Minimizing Coarse-Grained Locking
Ceph is a distributed storage system that is designed to be highly scalable and
resilient. It uses a number of techniques to minimize the use of coarse-grained locks,
which can be a major source of performance bottlenecks in distributed systems. One
of the key techniques used by Ceph is the use of a distributed hash table (DHT) to
partition the data across the cluster. This ensures that there is no contention for
access to the same data, as each piece of data is stored on a specific set of nodes.
Ceph also uses a technique called "copy-on-write" to avoid the need for locks when
updating data. When a piece of data is updated, a new copy of the data is created, and
the old copy is left untouched. This ensures that there is no contention for access to
the data, as each thread has its own copy. While Ceph is a distributed system, many of
its techniques can be adapted to a local RAID array. For example, the DHT could be
used to partition the data across the drives in the array, and the copy-on-write
technique could be used to avoid the need for locks when updating the data.
2.3.2. ScalaAFA's Message-Passing Based Permission Management
ScalaAFA is a user-space AFA that is built on top of SPDK. It uses a number of
innovative techniques to achieve a high level of performance and scalability. One of the
key techniques used by ScalaAFA is a message-passing based permission
management scheme. This scheme is used to coordinate access to the data and to
ensure that the array remains in a consistent state. When a write request is received,
the ScalaAFA engine sends a message to the drives in the array to request permission
to write the data. The drives then respond with a message indicating whether or not
they have granted permission. Once the engine has received permission from all the
drives, it can proceed with the write operation. This approach has the advantage of
being highly scalable, as it does not rely on any shared state between the drives.
However, it also requires a more complex design, as the engine must be able to handle
the asynchronous nature of message passing and the potential for messages to be lost
or reordered. While the full details of the ScalaAFA design are not publicly available, the
general principles of its permission management scheme can be used as a starting
point for the design of the new RAID bdev.
2.3.3. SSRAID's Stripe-Queued Architecture with Dedicated Threads
Generated by Kimi AI
SSRAID is a software RAID implementation that is designed to be highly scalable and
efficient. It uses a number of techniques to minimize the overhead of software RAID,
including a stripe-queued architecture with dedicated threads. In this architecture,
each stripe in the array has its own queue of I/O requests. When a request is received,
it is placed in the queue for the corresponding stripe. A dedicated thread then
processes the requests in the queue, one at a time. This approach has the advantage
of being highly scalable, as the number of threads can be increased to match the
number of stripes in the array. It also has the advantage of being highly efficient, as
there is no contention for access to the same stripe. The main challenge with this
approach is that it can be difficult to implement for a large number of stripes, as it
requires a large number of threads. However, by using a combination of a stripequeued architecture and a thread pool, it is possible to build a RAID bdev that is both
highly scalable and efficient.
3. Architectural Design of the New RAID Bdev in SPDK
The architectural design of the new RAID bdev must be a careful synthesis of the
principles discussed in the previous sections. It needs to be a high-performance, lockfree, and crash-consistent module that seamlessly integrates into the SPDK
ecosystem. The design should be modular, allowing for different RAID levels,
consistency mechanisms, and optimization strategies to be plugged in as needed. The
core of the design will be a set of components that work together to provide a robust
and efficient RAID implementation.
3.1. Core Components and Module Structure
The new RAID bdev will be implemented as a standard SPDK bdev module, which
means it will expose a block device interface to the rest of the SPDK stack. This will
allow it to be used by any SPDK application or target, such as an NVMe-oF target or
an iSCSI target. The module will be composed of several key components, each with a
specific responsibility.
3.1.1. Integrating with the SPDK Bdev Layer
The RAID bdev will be a standard SPDK bdev module, which means it will need to
implement the bdev interface. This interface defines a set of functions that must be
provided by all bdev modules, such as read , write , reset , and get_io_channel .
The RAID bdev will use this interface to receive I/O requests from the upper layers of
the SPDK stack and to send completion notifications back to them. The module will
Generated by Kimi AI
also need to register itself with the bdev layer so that it can be discovered and used by
other components of the system. This integration is crucial for ensuring that the RAID
bdev is a first-class citizen in the SPDK ecosystem and can be used in a wide variety
of applications.
3.1.2. Managing NVMe Devices via SPDK's NVMe Driver
The RAID bdev will rely on SPDK's NVMe driver to communicate with the underlying
NVMe devices. The NVMe driver provides a low-level interface for submitting I/O
requests to NVMe devices and for polling for completions. The RAID bdev will use this
interface to send read and write requests to the individual drives in the RAID array. The
module will need to be able to manage a set of NVMe devices, including opening them,
creating I/O queues, and handling errors. The use of SPDK's NVMe driver will ensure
that the RAID bdev can achieve the highest possible level of performance, as the driver
is designed to be lock-free and to minimize software overhead.
3.1.3. Structuring the RAID Logic (Striping, Parity Calculation)
The core of the RAID bdev will be the logic for implementing the RAID functionality.
This will include the code for striping data across the drives, calculating parity, and
handling read and write requests. The module will need to support multiple RAID levels,
such as RAID 0, 1, 5, and 6. The implementation of each RAID level will be a separate
component that can be plugged into the main RAID module. This will allow for a flexible
and extensible design, where new RAID levels can be added without modifying the core
of the module. The parity calculation will be a critical part of the RAID logic, and it will
need to be highly optimized to minimize its impact on performance.
3.2. Handling Parity and Metadata
The handling of parity and metadata is a critical aspect of the RAID bdev's design. The
parity information is essential for data recovery in the event of a drive failure, and the
metadata is necessary for ensuring consistency and for managing the state of the
array.
3.2.1. Offloading Parity Calculation to the Host CPU
In a traditional RAID implementation, the parity calculation is performed by the host
CPU. This is a simple and flexible approach, as it does not require any special
hardware support. The RAID bdev will include a highly optimized implementation of the
parity calculation algorithms, such as XOR for RAID 5 and Reed-Solomon for RAID 6.
Generated by Kimi AI
The module will use SIMD instructions, such as SSE or AVX, to accelerate the parity
calculation and to minimize its impact on performance. The use of the host CPU for
parity calculation will ensure that the RAID bdev is compatible with a wide range of
NVMe devices, as it does not require any special features from the drives.
3.2.2. Exploring SSD-Assisted Parity Offload (if available)
Some modern NVMe SSDs include hardware support for parity calculation. This can be
a significant performance advantage, as it can free up the host CPU to perform other
tasks. The RAID bdev will be designed to detect and use this feature if it is available.
This will involve sending special commands to the NVMe devices to request that they
perform the parity calculation. The module will need to be able to handle the case
where some drives support parity offloading and others do not. The use of SSDassisted parity offloading can significantly improve the performance of the RAID bdev,
especially for workloads with a high proportion of write operations.
3.2.3. Managing Metadata for Consistency and Recovery
The RAID bdev will need to manage a set of metadata to ensure consistency and to
support recovery. This metadata will include information about the state of the array,
such as which drives are part of the array, the RAID level, and the stripe size. It will also
include information about the state of each stripe, such as whether it is consistent or
not. This metadata will be stored on the NVMe devices, either in a dedicated metadata
area or in the OOB area of the drives. The module will need to be able to read and
write this metadata in a crash-safe manner, to ensure that the array can be recovered
correctly after a failure.
3.3. Interaction with the Flash Translation Layer (FTL)
The Flash Translation Layer (FTL) is a critical component of an NVMe SSD. It is
responsible for managing the flash memory, including wear leveling, garbage collection,
and bad block management. The design of the RAID bdev needs to take into account
the behavior of the FTL, as it can have a significant impact on performance and
reliability.
3.3.1. Understanding the Role of FTL in NVMe SSDs
The FTL is a firmware component that runs on the NVMe SSD. It presents a simple
block device interface to the host, while internally managing the complexities of the
flash memory. The FTL is responsible for mapping logical block addresses (LBAs) to
Generated by Kimi AI
physical pages in the flash memory. It also performs wear leveling to ensure that all
blocks of the flash memory are worn evenly, and it performs garbage collection to
reclaim space from invalid pages. The behavior of the FTL can have a significant
impact on the performance of the RAID bdev. For example, the garbage collection
process can cause write amplification, which can reduce the performance and
endurance of the SSD.
3.3.2. Potential for Host-Managed FTL (e.g., LightNVM)
In a traditional SSD, the FTL is managed by the device itself. However, there is a
growing trend towards host-managed FTLs, where the host system is responsible for
managing the flash memory. This can provide a number of advantages, such as better
performance, lower latency, and more control over the behavior of the flash memory.
The LightNVM project is an example of a host-managed FTL for NVMe SSDs. The
RAID bdev could be designed to work with a host-managed FTL, which would allow it
to have more control over the placement of data and parity on the flash memory. This
could lead to significant performance improvements, especially for workloads with a
high degree of random I/O.
3.3.3. Implications of FTL on Write Atomicity and Performance
The FTL can have a significant impact on the write atomicity and performance of the
RAID bdev. The FTL is responsible for ensuring that writes are atomic at the device
level. However, the RAID bdev needs to ensure that writes are atomic at the stripe level,
which spans multiple devices. The behavior of the FTL can also affect the performance
of the RAID bdev. For example, the garbage collection process can cause unpredictable
latency, which can make it difficult to meet performance targets. The RAID bdev will
need to be designed to work with the FTL to minimize the impact of these issues. This
could involve using techniques such as write coalescing and I/O scheduling to reduce
the impact of garbage collection and to improve the overall performance of the system.
4. Comparative Analysis of Existing RAID Implementations
To inform the design of a new, high-performance RAID bdev for SPDK, it is crucial to
conduct a thorough comparative analysis of existing RAID implementations. This
analysis should cover a range of systems, from the widely used Linux kernel software
RAID (mdraid) to more specialized research prototypes like ScalaAFA and stRAID. By
examining the strengths and weaknesses of these different approaches, we can identify
the key design principles and techniques that are most relevant to our goals of
Generated by Kimi AI
eliminating lock-induced stalls and ensuring data consistency. This comparative
analysis will serve as a guide for making informed design decisions and for avoiding
the pitfalls that have plagued previous RAID implementations.
4.1. Linux Kernel Software RAID (mdraid)
The Linux kernel's software RAID implementation, commonly known as mdraid, is a
mature and widely deployed solution for creating RAID arrays using standard block
devices. It supports a variety of RAID levels, including RAID 0, 1, 4, 5, 6, and 10, and
provides a rich set of features for managing and monitoring RAID volumes. However,
mdraid was designed in an era when hard disk drives were the dominant storage
medium, and its architecture is not always well-suited to the high-performance, lowlatency characteristics of modern NVMe SSDs. The use of coarse-grained locks and
the overhead of the kernel's block I/O stack can limit the performance of mdraid when
used with high-speed NVMe devices.
4.1.1. Strengths and Weaknesses with NVMe SSDs
When used with NVMe SSDs, mdraid exhibits a number of strengths and
weaknesses. Its primary strength is its maturity and robustness. It has been battletested in countless production environments and has a proven track record of
reliability. It provides a comprehensive set of features for managing RAID arrays,
including the ability to grow and shrink arrays, manage hot spares, and perform
detailed monitoring and diagnostics. It also has well-established mechanisms for
handling the write-hole problem, such as journaling and the Partial Parity Log (PPL),
which ensure data consistency even in the event of a crash .
However, the weaknesses of mdraid become apparent when it is pushed to its limits
with high-performance NVMe drives. The kernel's block I/O layer, which mdraid is
built upon, is not optimized for the low latency and high IOPS of NVMe devices. The
overhead of context switching between user space and kernel space, as well as the use
of interrupts for I/O completion, can become a significant bottleneck. Furthermore,
mdraid 's use of coarse-grained locks can limit its ability to scale on systems with
many CPU cores. While recent versions of mdraid have introduced optimizations
such as fine-grained locking and multi-threaded rebuilding, it is still fundamentally
limited by its architecture. A study on the performance of mdraid on high-end NVMe
Gen 5 drives showed that even with simplified configurations and various tuning
attempts, the performance did not scale as expected, indicating that the software
stack itself was the limiting factor . This highlights the need for a new approach, such
Generated by Kimi AI
as a user-space RAID implementation built on SPDK, to fully unlock the performance of
all-flash arrays.
4.1.2. Journaling and PPL Implementation Details
To address the write-hole problem, mdraid has evolved to include several softwarebased consistency mechanisms. The most well-known of these is the use of a
dedicated journal device, which was introduced to provide a write-ahead log for RAID 5
and 6 arrays . This approach, as discussed in Section 1.1.1, involves writing a
description of the intended changes to a separate device before applying them to the
main array. While effective, this method can introduce a performance bottleneck, as all
journal writes must go through the single journal device.
More recently, mdraid has adopted the Partial Parity Log (PPL) mechanism, which
offers a more efficient and scalable solution for RAID 5 arrays . The PPL is a
distributed log that is stored on the member drives of the array, eliminating the need
for a dedicated journal device. As described in Section 1.1.3, the PPL stores the partial
parity for each in-flight write operation, allowing the system to recover from a crash
without needing to perform a full resynchronization. The PPL is a significant
improvement over the dedicated journal approach, as it is more space-efficient and
does not create a single point of failure. The implementation of PPL in mdraid
demonstrates that it is possible to build a robust, software-only solution for the writehole problem that is suitable for use with standard NVMe SSDs.
4.1.3. Performance Bottlenecks and Locking Overheads
The performance bottlenecks in mdraid are not just limited to its kernel-space
operation and coarse-grained locking. The way it handles I/O can also be inefficient,
especially for small, random writes. In a RAID 5 or RAID 6 configuration, a small write
requires a read-modify-write (RMW) cycle, which involves reading the old data and old
parity, calculating the new parity, and then writing both the new data and the new
parity. This can be a very expensive operation, and mdraid's implementation is not
always optimized for the characteristics of flash-based storage. The comparison with
xiRAID Opus showed that mdraid's performance was six times worse for small block
writes, which is a common workload for many applications .
Furthermore, mdraid does not provide the same level of control over CPU affinity as a
user-space solution like SPDK. With mdraid, the RAID engine shares CPU resources
with other kernel processes and applications, which can lead to unpredictable
Generated by Kimi AI
performance and increased latency. In contrast, a user-space solution like xiRAID Opus
allows for the dedicated allocation of CPU cores to the RAID engine, ensuring that it
has the resources it needs to perform optimally . This level of control is crucial for
building a high-performance, predictable storage system. For the Kimi code project,
these findings underscore the importance of a user-space, lock-free architecture that
is specifically designed for the characteristics of NVMe SSDs. By avoiding the pitfalls
of mdraid, the new RAID bdev can deliver the high performance and low latency that is
expected from a modern all-flash array.
4.2. SPDK's Existing RAID Modules
The Storage Performance Development Kit (SPDK) includes a number of modules for
creating virtual block devices, including a RAID module. This module is designed to
provide a high-performance, user-space implementation of RAID that can be used to
aggregate multiple NVMe devices into a single logical volume. The SPDK RAID module
is built on top of SPDK's core principles of lock-free, asynchronous, and polled-mode
operation, which allows it to achieve much higher performance than traditional kernelbased RAID solutions. However, the existing SPDK RAID module is still under
development and has a number of limitations compared to mature solutions like
mdraid . Understanding these limitations is crucial for designing a new RAID bdev
that addresses these shortcomings while building on the high-performance foundation
of SPDK.
4.2.1. Analysis of the RAID5f Module
The primary RAID implementation in SPDK is the raid5f module. This module is
designed to provide a high-performance RAID 5 implementation that is optimized for
full-stripe writes. The "f" in raid5f stands for "full-stripe," which indicates its
primary design goal. The module is implemented in a lock-free manner, using the SPDK
reactor model to handle I/O on a per-core basis. This allows it to achieve very high
performance for workloads that are aligned to the stripe size and can be processed as
full-stripe writes. For these workloads, raid5f can deliver performance that is close
to the theoretical maximum for a software-only RAID implementation .
However, the raid5f module has a number of significant limitations. The most
notable is its lack of support for degraded mode operations. If a drive in the RAID 5
array fails, the raid5f module cannot handle reads or writes to the array, as it does
not have the logic to reconstruct data from the parity information. This makes it
unsuitable for most production environments, where the ability to tolerate a drive failure
Generated by Kimi AI
is a primary requirement of a RAID system. Another limitation is its focus on full-stripe
writes. While this is a good optimization for certain workloads, many real-world
applications generate smaller, random I/Os that do not align with the stripe size. The
raid5f module does not have an efficient way to handle these partial-stripe writes,
which can lead to poor performance for these workloads. Finally, the raid5f module
does not have any built-in mechanism for ensuring crash consistency, such as
journaling or a PPL. This means that it is vulnerable to the write-hole problem and
cannot guarantee data integrity in the event of a crash .
4.2.2. Limitations: Lack of Degraded Read and Rebuild
The lack of support for degraded read and rebuild operations is a major limitation of
the raid5f module. In a RAID 5 array, if a single drive fails, the array can continue to
operate in a degraded mode. Reads to the failed drive can be satisfied by reading the
corresponding data and parity from the remaining drives and reconstructing the
missing data. This allows the system to remain operational until the failed drive can be
replaced and the array rebuilt. The raid5f module does not implement this logic,
which means that a single drive failure renders the entire array inaccessible. This is a
critical missing feature that makes raid5f unsuitable for any application that requires
high availability.
Similarly, the raid5f module does not have a rebuild mechanism. Once a failed drive
is replaced, the data that was stored on that drive must be reconstructed from the
parity information on the other drives and written to the new drive. This process, known
as rebuilding, is a fundamental feature of any RAID implementation. The absence of a
rebuild mechanism in raid5f means that there is no way to restore the array to a fully
redundant state after a drive failure. These limitations are a direct result of the
module's focus on performance for a specific use case (full-stripe writes) at the
expense of the robustness and reliability features that are essential for a generalpurpose RAID solution. A new RAID bdev for SPDK would need to address these
limitations by implementing a full set of degraded mode operations and a robust rebuild
mechanism.
4.2.3. Performance Characteristics and Use Cases
Despite its limitations, the raid5f module does have some interesting performance
characteristics that are worth noting. As mentioned earlier, its performance is reported
to be very good, with some benchmarks showing performance that is close to that of a
raw device . This is due to its use of the write shaping technique, which can be very
Generated by Kimi AI
effective for certain workloads. The module is also designed to be simple and
lightweight, which means that it has a low CPU overhead and can be easily integrated
into a larger SPDK-based storage system.
Given these characteristics, the raid5f module may be suitable for certain use cases
where high performance is the primary concern and fault tolerance is not a major issue.
For example, it could be used in a development or testing environment, or in a noncritical application where data loss is not a major concern. It could also be used as a
starting point for a more advanced RAID implementation, providing a basic framework
that can be extended with additional features. However, for the Kimi code project,
which aims to improve the performance and reliability of the existing ScalaAFA system,
the raid5f module is not a suitable solution. The lack of support for degraded reads
and array rebuilds is a major drawback, and the write shaping technique is not a
general-purpose solution for all workloads. Instead, a new RAID module should be
developed that is based on a more robust and flexible architecture.
4.3. Commercial and Research Prototypes
In addition to the mainstream Linux kernel RAID, several research prototypes and
commercial products have explored new approaches to building high-performance
RAID systems. These systems often push the boundaries of what is possible with
software-defined storage, and their designs can provide valuable insights for the
development of the new RAID bdev. By studying these prototypes, we can learn about
the latest trends in storage system design and identify innovative techniques for
improving performance, scalability, and reliability.
4.3.1. ScalaAFA: A User-Space AFA in SPDK
ScalaAFA is a state-of-the-art, user-space AFA engine that is built on top of SPDK . It
is designed to address the challenges of building a high-performance AFA in a userspace environment, with a particular focus on eliminating lock-induced stalls and
leveraging the internal resources of modern SSDs. As discussed in Section 2.3.2,
ScalaAFA uses a message-passing-based permission management scheme to avoid
the use of locks, which allows it to achieve a high degree of parallelism and scalability.
The paper reports that this technique alone increases write throughput by 58.4% and
reduces average latency by 45.2% .
In addition to its lock-free architecture, ScalaAFA also introduces several other
innovative techniques. It proposes a novel data placement policy that allows it to
Generated by Kimi AI
offload the parity calculation to the built-in XOR engine of the SSDs, which reduces the
CPU overhead and improves performance. It also leverages the SSD's OOB (Out-ofBand) area to store critical metadata, such as the mapping table, which provides a
low-cost mechanism for ensuring crash consistency. The comprehensive evaluation of
ScalaAFA demonstrates that it can achieve 2.5x the write throughput and a 52.7%
reduction in average write latency compared to other state-of-the-art AFA engines .
The design of ScalaAFA provides a rich source of inspiration for the new RAID bdev,
and its techniques for achieving lock-free operation and leveraging SSD resources are
directly applicable to our goals.
4.3.2. xiRAID Opus: A High-Performance SPDK-Based RAID Engine
xiRAID Opus is a high-performance software RAID engine that is based on the SPDK
libraries and is designed for use in disaggregated infrastructures and virtualized
environments . It is a commercial product from Xinnor, a company that specializes in
high-performance storage solutions. xiRAID Opus is designed to operate in user space,
which allows it to bypass the kernel and achieve very low latency and high throughput.
It also provides a rich set of features, including support for NVMe over Fabrics (NVMeoF), iSCSI, and Vhost controllers, which makes it a very flexible and versatile solution .
The performance of xiRAID Opus is reported to be very good, with benchmarks
showing that it can achieve a significant performance improvement over traditional
kernel-based RAID solutions like mdraid. In a benchmark using PostgreSQL, xiRAID
Opus provided 30-40% better transaction per second (TPS) than mdraid in a selectonly workload. More significantly, in a degraded mode scenario, mdraid's performance
dropped by over 20x compared to xiRAID Opus . This demonstrates the robustness
and reliability of the xiRAID Opus architecture. For the Kimi code project, xiRAID Opus
provides a valuable benchmark and a source of inspiration. While the specific
implementation details of xiRAID Opus are not publicly available, the fact that it is
based on SPDK and achieves such high performance is a strong indication that a userspace, lock-free architecture is the right approach for a high-performance RAID bdev.
4.3.3. stRAID: A Run-to-Completion I/O Model for RAID
stRAID is another research prototype that focuses on improving the performance of
software RAID by reducing software overhead . The key innovation of stRAID is its use
of a run-to-completion (RTC) I/O processing scheme, which is designed to minimize
the number of context switches and handoffs between threads. In the RTC model, a
single thread is responsible for handling an I/O request from start to finish, which can
Generated by Kimi AI
significantly reduce the latency and CPU overhead of processing I/O requests. This
approach is particularly well-suited to the high-performance, low-latency environment
of an all-flash array, where even small amounts of software overhead can have a
significant impact on performance.
The stRAID paper provides a detailed analysis of the software overheads in traditional
kernel-based RAID implementations and demonstrates how the RTC model can be
used to mitigate these overheads. The paper also discusses the challenges of
implementing an RTC model in a multi-core environment and proposes several
techniques for managing concurrency and ensuring correctness. The lessons learned
from the stRAID project are highly relevant to the design of the new RAID bdev, as they
highlight the importance of minimizing software overhead and provide a practical
framework for achieving this goal. By adopting an RTC model, the new RAID bdev can
be designed to be both highly performant and scalable, without being hampered by the
overhead of traditional I/O processing models.
